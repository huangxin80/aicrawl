#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DrissionPage çˆ¬è™«æœåŠ¡ - Plan B åç«¯
æä¾› HTTP API æ¥å£ä¾› Node.js è°ƒç”¨
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import os
import time
import uuid
from datetime import datetime
import traceback
import threading
import logging

try:
    from DrissionPage import ChromiumPage, ChromiumOptions
    DRISSIONPAGE_AVAILABLE = True
except ImportError:
    DRISSIONPAGE_AVAILABLE = False
    print("è­¦å‘Š: DrissionPage æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install DrissionPage")

app = Flask(__name__)
CORS(app)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DrissionPageCrawler:
    def __init__(self):
        self.sessions = {}  # å­˜å‚¨æ´»è·ƒçš„æµè§ˆå™¨ä¼šè¯
        self.catch_dir = "D:\\crawler\\crawler\\catch"
        self.ensure_catch_directory()
    
    def ensure_catch_directory(self):
        """ç¡®ä¿catchç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.catch_dir):
            os.makedirs(self.catch_dir, exist_ok=True)
            logger.info(f"åˆ›å»ºcatchç›®å½•: {self.catch_dir}")
    
    def create_browser_options(self):
        """åˆ›å»ºæµè§ˆå™¨é…ç½® - å¢å¼ºåæ£€æµ‹"""
        co = ChromiumOptions()
        
        # åŸºç¡€è®¾ç½®
        co.headless(False)  # å¯è§æ¨¡å¼ï¼Œä¾¿äºè°ƒè¯•
        co.no_imgs()       # ä¸åŠ è½½å›¾ç‰‡ï¼Œæå‡é€Ÿåº¦
        co.no_js()         # å¯é€‰ï¼šç¦ç”¨JavaScriptï¼ˆæ ¹æ®éœ€è¦å¼€å¯ï¼‰
        co.mute()          # é™éŸ³
        
        # å¢å¼ºåæ£€æµ‹å‚æ•°
        co.set_argument('--disable-blink-features=AutomationControlled')
        co.set_argument('--disable-web-security')
        co.set_argument('--ignore-certificate-errors')
        co.set_argument('--ignore-ssl-errors')
        co.set_argument('--allow-running-insecure-content')
        co.set_argument('--no-sandbox')
        co.set_argument('--disable-dev-shm-usage')
        co.set_argument('--disable-gpu')
        co.set_argument('--disable-extensions')
        co.set_argument('--disable-plugins')
        co.set_argument('--disable-images')  # DrissionPageç‰¹æœ‰
        
        # ç”¨æˆ·ä»£ç†
        co.set_user_agent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # è®¾ç½®çª—å£å¤§å°
        co.set_window_size(1920, 1080)
        
        return co
    
    def create_session(self) -> str:
        """åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¼šè¯"""
        if not DRISSIONPAGE_AVAILABLE:
            raise Exception("DrissionPage æœªå®‰è£…")
        
        session_id = str(uuid.uuid4())
        co = self.create_browser_options()
        
        try:
            page = ChromiumPage(addr_or_opts=co)
            self.sessions[session_id] = {
                'page': page,
                'created_at': datetime.now(),
                'last_used': datetime.now()
            }
            logger.info(f"åˆ›å»ºæµè§ˆå™¨ä¼šè¯: {session_id}")
            return session_id
        except Exception as e:
            logger.error(f"åˆ›å»ºæµè§ˆå™¨ä¼šè¯å¤±è´¥: {str(e)}")
            raise
    
    def close_session(self, session_id: str):
        """å…³é—­æµè§ˆå™¨ä¼šè¯"""
        if session_id in self.sessions:
            try:
                self.sessions[session_id]['page'].quit()
                del self.sessions[session_id]
                logger.info(f"å…³é—­æµè§ˆå™¨ä¼šè¯: {session_id}")
            except Exception as e:
                logger.error(f"å…³é—­ä¼šè¯å¤±è´¥: {str(e)}")
    
    def get_page(self, session_id: str):
        """è·å–é¡µé¢å¯¹è±¡"""
        if session_id not in self.sessions:
            raise Exception(f"ä¼šè¯ä¸å­˜åœ¨: {session_id}")
        
        session = self.sessions[session_id]
        session['last_used'] = datetime.now()
        return session['page']
    
    def save_js_file(self, content: str, url: str) -> str:
        """ä¿å­˜JSæ–‡ä»¶åˆ°æœ¬åœ°"""
        try:
            from urllib.parse import urlparse
            import os
            
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            parsed_url = urlparse(url)
            hostname = parsed_url.hostname or 'unknown'
            hostname = hostname.replace('.', '_').replace(':', '_')
            
            timestamp = int(time.time() * 1000)
            filename = f"{hostname}_{timestamp}_drissionpage.js"
            
            filepath = os.path.join(self.catch_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"ä¿å­˜JSæ–‡ä»¶: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return ""
    
    def enhanced_page_analysis(self, page) -> dict:
        """å¢å¼ºç‰ˆé¡µé¢åˆ†æ"""
        try:
            # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
            title = page.title
            url = page.url
            
            # è·å–é¡µé¢å†…å®¹
            try:
                text_content = page.ele('tag:body').text if page.ele('tag:body', timeout=2) else ""
            except:
                text_content = ""
            
            try:
                html_content = page.html
            except:
                html_content = ""
            
            # æ£€æµ‹JavaScriptæ¡†æ¶
            js_frameworks = []
            if 'react' in html_content.lower() or page.ele('[data-reactroot]', timeout=1):
                js_frameworks.append('React')
            if 'vue' in html_content.lower() or page.ele('[data-v-]', timeout=1):
                js_frameworks.append('Vue')
            if 'angular' in html_content.lower() or page.ele('[ng-app]', timeout=1):
                js_frameworks.append('Angular')
            
            # å†…å®¹åˆ†æ
            content_score = 0
            if text_content:
                content_score += min(len(text_content) / 100, 50)
            if html_content:
                content_score += min(len(html_content) / 1000, 30)
            
            # æ£€æµ‹åŠ è½½æŒ‡ç¤ºå™¨
            loading_indicators = []
            loading_selectors = ['.loading', '.spinner', '.skeleton', '[class*="loading"]']
            for selector in loading_selectors:
                if page.ele(selector, timeout=1):
                    loading_indicators.append(selector)
            
            return {
                'title': title,
                'url': url,
                'text_length': len(text_content) if text_content else 0,
                'html_length': len(html_content) if html_content else 0,
                'has_content': len(text_content) > 50 if text_content else False,
                'js_frameworks': js_frameworks,
                'is_js_app': len(js_frameworks) > 0,
                'content_score': content_score,
                'loading_indicators': loading_indicators,
                'is_stable': len(loading_indicators) == 0
            }
        except Exception as e:
            logger.error(f"é¡µé¢åˆ†æå¤±è´¥: {str(e)}")
            return {
                'title': '',
                'url': url if 'url' in locals() else '',
                'text_length': 0,
                'html_length': 0,
                'has_content': False,
                'js_frameworks': [],
                'is_js_app': False,
                'content_score': 0,
                'loading_indicators': [],
                'is_stable': False,
                'error': str(e)
            }
    
    def smart_wait(self, page, max_wait_time: int = 15) -> dict:
        """æ™ºèƒ½ç­‰å¾…æœºåˆ¶"""
        logger.info("å¼€å§‹DrissionPageæ™ºèƒ½ç­‰å¾…...")
        
        start_time = time.time()
        last_content_length = 0
        stable_count = 0
        
        while (time.time() - start_time) < max_wait_time:
            try:
                # åˆ†æå½“å‰é¡µé¢çŠ¶æ€
                analysis = self.enhanced_page_analysis(page)
                current_content_length = analysis['text_length']
                
                # æ£€æŸ¥å†…å®¹ç¨³å®šæ€§
                if abs(current_content_length - last_content_length) < 50:
                    stable_count += 1
                else:
                    stable_count = 0
                
                last_content_length = current_content_length
                
                # å¦‚æœå†…å®¹ç¨³å®šä¸”æ²¡æœ‰åŠ è½½æŒ‡ç¤ºå™¨
                if stable_count >= 3 and analysis['is_stable'] and analysis['has_content']:
                    logger.info("é¡µé¢å†…å®¹å·²ç¨³å®š")
                    break
                
                # å¦‚æœæ£€æµ‹åˆ°JSæ¡†æ¶ä½†å†…å®¹ä¸è¶³ï¼Œå°è¯•è§¦å‘
                if analysis['is_js_app'] and not analysis['has_content']:
                    logger.info("æ£€æµ‹åˆ°JSåº”ç”¨ï¼Œå°è¯•è§¦å‘å†…å®¹...")
                    self.trigger_js_content(page)
                
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"æ™ºèƒ½ç­‰å¾…è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                break
        
        final_analysis = self.enhanced_page_analysis(page)
        logger.info(f"æ™ºèƒ½ç­‰å¾…å®Œæˆï¼Œæœ€ç»ˆå¾—åˆ†: {final_analysis['content_score']}")
        return final_analysis
    
    def trigger_js_content(self, page):
        """è§¦å‘JavaScriptå†…å®¹"""
        try:
            # æ»šåŠ¨é¡µé¢
            page.scroll.to_bottom()
            time.sleep(1)
            page.scroll.to_top()
            time.sleep(1)
            
            # å°è¯•ç‚¹å‡»å¸¸è§çš„åŠ è½½æŒ‰é’®
            load_buttons = ['button:contains("åŠ è½½")', 'button:contains("æ›´å¤š")', '.load-more', '.show-more']
            for selector in load_buttons:
                try:
                    btn = page.ele(selector, timeout=1)
                    if btn:
                        btn.click()
                        time.sleep(2)
                        break
                except:
                    continue
                    
        except Exception as e:
            logger.error(f"è§¦å‘JSå†…å®¹å¤±è´¥: {str(e)}")
    
    def capture_files_and_urls(self, session_id: str, target_url: str) -> dict:
        """æ•è·æ–‡ä»¶å’ŒURL - DrissionPageç‰ˆæœ¬"""
        try:
            page = self.get_page(session_id)
            
            # å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            logger.info(f"DrissionPageè®¿é—®: {target_url}")
            page.get(target_url)
            
            # æ™ºèƒ½ç­‰å¾…
            analysis = self.smart_wait(page)
            
            # æ”¶é›†JavaScriptæ–‡ä»¶
            js_files = []
            try:
                # DrissionPageè·å–æ‰€æœ‰scriptæ ‡ç­¾
                scripts = page.eles('tag:script')
                for i, script in enumerate(scripts):
                    src = script.attr('src')
                    if src and (src.endswith('.js') or 'javascript' in src):
                        try:
                            # è·å–JSæ–‡ä»¶å†…å®¹ï¼ˆéœ€è¦å¦å¤–è¯·æ±‚ï¼‰
                            js_response = page.get(src, retry=2, interval=1)
                            if js_response and hasattr(js_response, 'text'):
                                content = js_response.text
                                local_path = self.save_js_file(content, src)
                                
                                js_files.append({
                                    'url': src,
                                    'content': content,
                                    'size': len(content),
                                    'local_path': local_path,
                                    'method': 'GET',
                                    'timestamp': int(time.time() * 1000)
                                })
                        except Exception as e:
                            logger.error(f"è·å–JSæ–‡ä»¶å¤±è´¥ {src}: {str(e)}")
            except Exception as e:
                logger.error(f"æ”¶é›†JSæ–‡ä»¶å¤±è´¥: {str(e)}")
            
            # æ”¶é›†æ‰€æœ‰URLï¼ˆåŸºäºé¡µé¢é“¾æ¥ï¼‰
            urls = []
            try:
                # è·å–æ‰€æœ‰é“¾æ¥
                links = page.eles('tag:a')
                for link in links:
                    href = link.attr('href')
                    if href:
                        urls.append({
                            'url': href,
                            'method': 'GET',
                            'status': 200,  # DrissionPageæ— æ³•ç›´æ¥è·å–çŠ¶æ€ç 
                            'content_type': 'text/html',
                            'url_type': 'other',
                            'is_api': '/api/' in href.lower() or '/v1/' in href.lower(),
                            'timestamp': int(time.time() * 1000)
                        })
            except Exception as e:
                logger.error(f"æ”¶é›†URLå¤±è´¥: {str(e)}")
            
            return {
                'success': True,
                'files': js_files,
                'urls': urls,
                'page_analysis': analysis,
                'routes': [],  # DrissionPageæš‚ä¸æ”¯æŒSPAè·¯ç”±æ£€æµ‹
                'engine': 'DrissionPage'
            }
            
        except Exception as e:
            logger.error(f"DrissionPageæ•è·å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'files': [],
                'urls': [],
                'page_analysis': {},
                'routes': [],
                'engine': 'DrissionPage'
            }

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = DrissionPageCrawler()

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'drissionpage_available': DRISSIONPAGE_AVAILABLE,
        'active_sessions': len(crawler.sessions),
        'timestamp': datetime.now().isoformat()
    })

@app.route('/session/create', methods=['POST'])
def create_session():
    """åˆ›å»ºæµè§ˆå™¨ä¼šè¯"""
    try:
        session_id = crawler.create_session()
        return jsonify({
            'success': True,
            'session_id': session_id
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/session/<session_id>/close', methods=['POST'])
def close_session(session_id):
    """å…³é—­æµè§ˆå™¨ä¼šè¯"""
    try:
        crawler.close_session(session_id)
        return jsonify({
            'success': True
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/crawl', methods=['POST'])
def crawl_website():
    """çˆ¬å–ç½‘ç«™ - ä¸»è¦APIæ¥å£"""
    try:
        data = request.get_json()
        target_url = data.get('url')
        
        if not target_url:
            return jsonify({
                'success': False,
                'error': 'URL is required'
            }), 400
        
        # åˆ›å»ºä¸´æ—¶ä¼šè¯
        session_id = crawler.create_session()
        
        try:
            # æ‰§è¡Œçˆ¬å–
            result = crawler.capture_files_and_urls(session_id, target_url)
            
            return jsonify(result)
            
        finally:
            # æ¸…ç†ä¼šè¯
            crawler.close_session(session_id)
            
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'files': [],
            'urls': [],
            'page_analysis': {},
            'routes': [],
            'engine': 'DrissionPage'
        }), 500

@app.route('/test', methods=['POST'])
def test_connection():
    """æµ‹è¯•è¿æ¥å’ŒDrissionPageåŠŸèƒ½"""
    try:
        data = request.get_json()
        test_url = data.get('url', 'https://httpbin.org/html')
        
        if not DRISSIONPAGE_AVAILABLE:
            return jsonify({
                'success': False,
                'error': 'DrissionPage not available'
            })
        
        # å¿«é€Ÿæµ‹è¯•
        session_id = crawler.create_session()
        
        try:
            page = crawler.get_page(session_id)
            page.get(test_url, timeout=10)
            
            analysis = crawler.enhanced_page_analysis(page)
            
            return jsonify({
                'success': True,
                'message': 'DrissionPage working correctly',
                'test_url': test_url,
                'analysis': analysis
            })
            
        finally:
            crawler.close_session(session_id)
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/sessions', methods=['GET'])
def list_sessions():
    """åˆ—å‡ºæ´»è·ƒä¼šè¯"""
    sessions_info = {}
    for sid, session in crawler.sessions.items():
        sessions_info[sid] = {
            'created_at': session['created_at'].isoformat(),
            'last_used': session['last_used'].isoformat()
        }
    
    return jsonify({
        'active_sessions': len(crawler.sessions),
        'sessions': sessions_info
    })

def cleanup_old_sessions():
    """æ¸…ç†è¶…æ—¶çš„ä¼šè¯"""
    while True:
        try:
            current_time = datetime.now()
            timeout_sessions = []
            
            for session_id, session in crawler.sessions.items():
                if (current_time - session['last_used']).seconds > 300:  # 5åˆ†é’Ÿè¶…æ—¶
                    timeout_sessions.append(session_id)
            
            for session_id in timeout_sessions:
                logger.info(f"æ¸…ç†è¶…æ—¶ä¼šè¯: {session_id}")
                crawler.close_session(session_id)
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä¼šè¯å¤±è´¥: {str(e)}")
        
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

# å¯åŠ¨æ¸…ç†çº¿ç¨‹
cleanup_thread = threading.Thread(target=cleanup_old_sessions, daemon=True)
cleanup_thread.start()

if __name__ == '__main__':
    print("ğŸš€ DrissionPage çˆ¬è™«æœåŠ¡å¯åŠ¨ä¸­...")
    print(f"ğŸ“ æ–‡ä»¶ä¿å­˜ç›®å½•: {crawler.catch_dir}")
    print(f"ğŸ”§ DrissionPage å¯ç”¨: {DRISSIONPAGE_AVAILABLE}")
    
    if not DRISSIONPAGE_AVAILABLE:
        print("âš ï¸  è¯·å®‰è£… DrissionPage: pip install DrissionPage")
    
    app.run(host='127.0.0.1', port=5000, debug=False, threaded=True) 